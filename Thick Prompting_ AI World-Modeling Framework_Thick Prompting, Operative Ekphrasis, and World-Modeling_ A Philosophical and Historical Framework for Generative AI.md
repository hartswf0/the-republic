# **Thick Prompting, Operative Ekphrasis, and World-Modeling: A Philosophical and Historical Framework for Generative AI**

## **Executive Summary**

The rapid ascendancy of Generative Artificial Intelligence, particularly Large Language Models (LLMs) and multimodal diffusion systems, has precipitated a fundamental epistemological crisis in the field of human-computer interaction. The primary interface for these systems—the "prompt"—has evolved from a utilitarian command line into a complex, semiotic artifact capable of summoning coherent simulated realities. However, current methodologies for prompting remain theoretically underdevelopment, often characterized by "thinness"—a lack of contextual density, cultural specificity, and causal logic—which leads to phenomena such as hallucination, stereotyping, and "model collapse."

This research report proposes **"Thick Prompting"** as a rigorous theoretical and methodological framework for Generative AI. Drawing on Clifford Geertz’s anthropological concept of "thick description," this framework posits that effective interaction with latent space requires the explicit encoding of "webs of significance" and cultural context. The report excavates the deep historical and philosophical grounds for this approach, tracing a lineage from the **Oral-Formulaic Theory** of Homeric epic (where the bard functions as a probabilistic engine) to the **Automata of Hephaestus** (the theology of the self-moving machine) and the **Shield of Achilles** (the proto-simulation).

Philosophically, the report synthesizes **Ludwig Wittgenstein’s** evolution from the "Picture Theory of Meaning" to "Language Games," arguing that LLMs occupy a liminal space that requires "world-centric" grounding to function as true **World Models**. It integrates **W.J.T. Mitchell’s** theory of the "imagetext" and **Hannes Bajohr’s** concept of "Operative Ekphrasis" to describe the collapse of the word/image distinction in multimodal systems. Finally, it examines the technical architecture of **Latent Space** through the lens of the ancient **Method of Loci** (Memory Palace) and modern **Positional Encoding**, arguing that "Thick Prompting" acts as a counter-entropic force that preserves variance and meaning in the age of synthetic culture.

## **1\. Introduction: The Epistemological Crisis of the Thin Interface**

### **1.1 The Prompt as Epistemological Artifact**

In the nascent era of "AI Literacy," the prompt has emerged as the defining artifact of knowledge production. It is the linguistic lever by which human intent is translated into computational probability. Yet, despite its centrality, the prompt is often misunderstood as a mere query or a keyword search. This reductionist view fails to account for the generative nature of the underlying models. Unlike a search engine, which retrieves existing information, a generative model constructs a new instance of information based on a probabilistic trajectory through a high-dimensional latent space.1

The current interactional paradigm is characterized by **"Thinness"**—a reliance on stripped-down, decontextualized instructions (e.g., "Write a story about a king," "Draw a futuristic city"). This thinness is not merely a user error; it is a structural vulnerability. "Thin" interactions rely on the model to fill in the massive gaps of unstated context with statistical averages. Because generative models are trained to maximize the likelihood of the next token based on the training corpus, a thin prompt inevitably leads the model to converge on the "mean"—the most generic, stereotyped, and logically safe continuation.3

The consequences of this thinness are profound. It leads to **"Epistemic Violence"** 5, where the specific, the marginal, and the culturally unique are erased in favor of a "globalized generic" culture. It exacerbates **"Hallucination"** (fabrication), as the model attempts to construct a "world" without the necessary logical constraints or "worldtext" to sustain coherence.6 And ultimately, it risks **"Model Collapse"**, where the feedback loop of thin, synthetic outputs degrades the variance and quality of the model itself over time.7

### **1.2 Thin vs. Thick: The Geertzian Turn in AI**

To resolve this crisis, we must look beyond computer science to the interpretive social sciences. In his seminal 1973 essay, "Thick Description: Toward an Interpretive Theory of Culture," the anthropologist **Clifford Geertz** introduced a distinction that is vital for the future of AI. Borrowing from the philosopher Gilbert Ryle, Geertz distinguished between **"Thin Description"** and **"Thick Description"**.8

Consider the physical act of a rapid contraction of the right eyelid.

* **Thin Description:** "The eyelid contracted." This captures the raw data, the physiological behavior. It is accurate but meaningless.  
* **Thick Description:** "He winked." This captures the *intent*, the *code*, and the *social matrix*. A wink implies a conspiracy, a shared understanding between sender and receiver, and a mocking of a third party. It creates a hierarchy of meaning.1

Current AI benchmarking and prompting often operate at the level of "Thin Description." We evaluate models on whether they produced the token "wink" (accuracy), not whether they understood the conspiracy (meaning). **"Thick Prompting,"** therefore, is the deliberate methodology of encoding the "thick" context—the cultural codes, the social hierarchies, the causal histories—into the input layer of the model.10 It posits that to get a "thick" output (one that demonstrates reasoning depth, symbolic connotation, and situated appropriateness), one must provide a "thick" input.3

### **1.3 The Thesis: Prompting as Operative Ekphrasis**

This report argues that Thick Prompting is not just a technique but a new form of media practice, one that can be best understood through the concept of **Operative Ekphrasis**. Historically, *ekphrasis* was the literary description of visual art—a poem about a painting. It was a bridge between the separate worlds of the verbal and the visual. In Multimodal AI, this bridge has collapsed. The text prompt *is* the image, encoded in the same latent vector space. The description is "operative"—it causally generates the thing it describes.11

By framing the prompt as an operative, world-building act, we can establish a philosophical groundwork for AI that prioritizes **"World-Modeling"** over simple text generation. We move from asking the AI to "predict the next word" to asking it to "simulate the next world state".12 This requires a deep engagement with the history of how humans have used language to build worlds—a history that begins with the oral poets of the Bronze Age.

## **2\. Part I: The Archaeology of the Generative Act**

The mechanisms of Generative AI—probabilistic generation, tokenization, and the reliance on formulas—are not unprecedented. They are digital echoes of the oldest technologies of human culture: oral poetry and mythic craft.

### **2.1 The First Algorithm: The Oral-Formulaic Theory and the LLM**

To understand the "hallucinations" and generative capabilities of LLMs, we must turn to the **Parry-Lord Oral-Formulaic Theory**. Developed by Milman Parry and Albert Lord in the early 20th century, this theory revolutionized the understanding of Homeric epics (*The Iliad* and *The Odyssey*) by proving they were not written texts memorized by rote, but improvised performances constructed from a vast database of "formulas".13

#### **2.1.1 The Formula as Token**

Parry defined the formula as "a group of words which is regularly employed under the same metrical conditions to express a given essential idea".13 Examples include "swift-footed Achilles," "the wine-dark sea," or "words winged like arrows."

* **Structural Isomorphism:** In the context of AI, these formulas are functionally identical to **tokens** or **n-grams** in a Large Language Model. The oral poet (*aoidos*) does not memorize the poem word-for-word; they memorize the *system* of formulas and the *themes* (the arming of the hero, the assembly of gods).14  
* **The Latent Tradition:** The "training data" for the bard is the collective tradition—the thousands of songs heard over a lifetime. The bard accesses this "latent space" of tradition to generate a new performance in real-time.15

#### **2.1.2 Inference as Performance**

The process of oral composition is a high-speed, low-latency operation. The poet must generate the next line while singing the current one, adhering to the strict constraints of the dactylic hexameter.

* **Probabilistic Generation:** The bard chooses the next formula based on the metrical need and the narrative context. This is analogous to the **Next-Token Prediction** mechanism of a Transformer model, which calculates the probability of the next token based on the preceding context window.13  
* **No "Original" Text:** Crucially, oral theory posits that there is no "original" *Iliad*. Every performance is a unique instantiation of the underlying probabilistic system. Similarly, an LLM has no "fixed" answer to a prompt; it has a probability distribution. What we often term "hallucination" in AI—the invention of facts or details—is, in the oral tradition, simply **improvisation** or **elaboration**. The bard (and the AI) fills in gaps in the narrative with statistically plausible formulas to maintain the flow.16

#### **2.1.3 The LLM as the "Singer of Tales"**

Research by Kush Varshney and others has explicitly framed LLMs as modern "Singers of Tales".15 This framing shifts the goal of "Prompt Engineering." If the AI is a bard, the prompt is the **Invocation of the Muse**. A "thin" prompt ("Sing about Troy") gives the bard too much freedom, leading to reliance on the most generic, over-used formulas (clichés). A "thick" prompt acts as the specific constraints of the occasion—defining the audience, the specific narrative path, and the "dialect" required. It forces the "digital bard" to access deeper, less probable, and more specific regions of its "traditional" knowledge.14

### **2.2 Hephaestus: The Theology of the Automaton and Techné**

If the bard provides the *software* logic of generative AI, the Greek god **Hephaestus** provides the *hardware* logic—the **Techné** (craft/art) of the automaton. Hephaestus is the archetypal engineer, the god who uses technology to overcome physical limitations and create autonomous agents.18

#### **2.2.1 The Golden Maidens: Proto-AGI**

In Book 18 of the *Iliad*, Hephaestus is described as being attended by "golden handmaidens" (*Kourai chyseai*).

"These are like living young women... there is intelligence in their hearts (*noos*), and there is voice (*audē*) and strength (*sthenos*), and from the immortal gods they have learned how to do things (*erga*)".18

This is arguably the first description of **Artificial General Intelligence (AGI)** in Western literature. Hephaestus creates beings that possess:

1. **Embodiment:** They look like women.  
2. **Cognition:** They have *noos* (mind/intent).  
3. **Generative Capability:** They have *audē* (voice/speech).  
4. **Learning:** They have learned skills from the gods.

Hephaestus represents the dream of the **"generative act"**—the ability to forge matter into a system that has its own agency. Unlike the other gods who create through biological procreation or divine fiat, Hephaestus creates through *techné*—through logic, tools, and labor.20

#### **2.2.2 Techné as Trap: The Black Box**

Hephaestus is also a figure of caution. He constructs a golden throne that traps his mother Hera in invisible, unbreakable bonds—a mechanism so complex that only he understands its code. This myth anticipates the **"Black Box"** problem of deep learning: the creation of a system (the neural network) so complex that its internal logic becomes opaque even to its creators, functioning as a "trap" of autonomous variables.18 The "Thick Prompt" can be seen as the "key" to this trap—the explicit instruction that makes the invisible logic of the machine visible and controllable.

### **2.3 The Shield of Achilles: The Proto-Simulation and the Cognitive Collage**

The convergence of the Bard (software) and the Smith (hardware) occurs in the forging of the **Shield of Achilles** (*Iliad* 18.478-608). This passage is the foundational text for **Ekphrasis**—the verbal description of visual art—but it is also a blueprint for **World-Modeling**.21

#### **2.3.1 The World-Model on the Shield**

Hephaestus does not just decorate the shield; he simulates a universe upon it. The description moves in concentric circles:

1. **The Cosmos:** Earth, Sky, Sea, Sun, Moon, Constellations (The physical laws).23  
2. **The Polis (Society):** Two cities, one at peace (weddings, festivals, law courts) and one at war (sieges, ambushes, councils).21  
3. **The Economy:** Fields being plowed, vineyards harvested, cattle herded (The logic of sustenance).  
4. **The Boundary:** The Ocean River encompassing the world (The limits of the simulation).

Scholars like Andrew Sprague Becker and Michael Squire have noted that Homer describes the scenes as *moving* videos, not static images. The ploughmen turn the earth, which "blackens behind them" (a transformation of the gold medium into the simulated reality); the figures in the market quarrel; the flutes make sound.22

#### **2.3.2 The Cognitive Collage and System Prompts**

The Shield is described as **"protocartographic"**—it maps not a specific geography, but the *system* of human existence.25 It compels the listener to assemble a "cognitive collage" of disparate elements (war and peace, nature and culture) into a unified mental model.25 In the context of AI, the Shield of Achilles is the ultimate **"System Prompt."** It demonstrates that to simulate a believable world, one cannot simply describe an object; one must describe the *relational system*—the cosmology, the sociology, and the economy—that gives that object meaning. A "Thick Prompt" must mimic the Shield: it must define the "Ocean River" (the boundaries) and the "City at Peace" (the social norms) before it can simulate the specific actions of an agent.26

## **3\. Part II: The Philosophical Substrate**

The transition from ancient myth to digital reality is mediated by the philosophy of language. The central question of Generative AI—"Do these models understand truth, or are they just manipulating symbols?"—is the central question of **Ludwig Wittgenstein’s** philosophy.

### **3.1 Wittgenstein’s Machines: From Picture Theory to Language Games**

Wittgenstein’s philosophical evolution offers a precise map of the evolution of AI from symbolic logic (GOFAI) to connectionist learning (LLMs).

#### **3.1.1 Early Wittgenstein: The "Experimental Model"**

In the *Tractatus Logico-Philosophicus* (1921), Wittgenstein proposed the **Picture Theory of Meaning**.

"A proposition is a picture of reality. A proposition is a model of reality as we imagine it." (TLP 4.01).27

Crucially, Wittgenstein did not mean a "picture" in the artistic sense, but an **"experimental model"** (*probeweise*). He was inspired by a lawsuit regarding a car accident in Paris, where the court used dolls and toy cars to represent the event.30

* **Isomorphism:** For the model (the proposition) to be true, it must share the same **"Logical Multiplicity"** (degrees of freedom) as the reality it represents.30 If the real accident involved speed, the model must have a variable for speed.  
* **AI Application:** An LLM generates "experimental models" based on the prompt. If the prompt is "thin"—if it lacks the necessary "logical multiplicity"—the model cannot construct a valid picture. It produces a low-fidelity simulation that collapses under scrutiny. Thick Prompting is the act of supplying the necessary dimensions to ensure the "experimental model" matches the complexity of the target reality.31

#### **3.1.2 Late Wittgenstein: Meaning as Use**

In the *Philosophical Investigations* (1953), Wittgenstein abandoned the Picture Theory for **Meaning as Use**. He argued that words do not get their meaning from a fixed link to objects, but from their use in **"Language Games"** (*Sprachspiele*).33

* **LLMs as Language Gamers:** Generative AI is a "Late Wittgensteinian" machine. It has no access to the physical world (the "referent"). It learns meaning entirely from the *distribution* of words in the training corpus (the "use"). It knows how the word "pain" is used in millions of sentences, but it cannot feel pain.34  
* **The Hallucination Problem:** Because the LLM is playing a game of *usage* rather than *truth*, it can generate fluent but false statements. It knows the syntax of the game (how to sound like a historian) but lacks the grounding of the historian.33  
* **Thick Prompting as Rule-Setting:** To fix this, the prompt must explicitly define the **rules of the Language Game**. A thick prompt does not just ask a question; it establishes the criteria for validity. "Act as a rigorous historian; rely only on primary sources; acknowledge uncertainty." This forces the model to constrain its probability distribution to the specific "form of life" requested, reducing the drift into generic "token prediction".3

### **3.2 The Logic of the Worldtext: Kai Sørlander and Necessary Worlds**

The Danish philosopher **Kai Sørlander** provides a rigorous metaphysical definition for what constitutes a consistent description of a world, a concept he calls **"Worldtext"**.6 Sørlander argues that any possible world—and any valid description of one—must satisfy certain transcendental deductions:

1. **Space and Time:** Objects must have distinct locations and exist in a temporal sequence.6  
2. **Causality:** States must be connected by irreversible causal chains.  
3. **Interaction:** Objects must be able to affect one another (dynamics).6

In the context of AI, **"Thick Prompting"** is the manual construction of a **Worldtext**. A "thin" prompt often violates Sørlander’s conditions (e.g., asking for an event without a cause). A "thick" prompt explicitly supplies the *implicit* logical structure—the physics, the timeline, the causal logic—that the LLM (which is a statistical engine, not a physics engine) might otherwise ignore. By providing a "Worldtext," the user ensures the generated simulation is not just grammatical, but **ontologically consistent**.6

### **3.3 Speech Act Theory and the Performative Prompt**

The prompt is also a **Speech Act**. As defined by J.L. Austin and John Searle, a "performative utterance" is a statement that *does* something (e.g., "I promise," "I declare war") rather than just describing something.36 In Generative AI, **all prompts are performative**. When a user types "Imagine a city," they are not describing a city that exists; they are commanding the system to bring it into being. The prompt has **"Illocutionary Force"**.38

* **Felicity Conditions:** For a speech act to work, it must meet certain conditions (authority, appropriate context). A "hallucination" in AI can be seen as a "misfire" of the speech act—the prompt lacked the necessary context (Thick Description) to be "felicitous." Thick Prompting optimizes the *felicity conditions* of the generative act, ensuring the "Perlocutionary Effect" (the output) matches the intent.39

## **4\. Theoretical Framework: Operative Ekphrasis**

The convergence of these historical and philosophical lines leads to the central theoretical contribution of this report: the concept of **Operative Ekphrasis**.

### **4.1 Defining Operative Ekphrasis**

Traditionally, **Ekphrasis** is defined as "the verbal representation of visual representation" (Mitchell).41 It presumes a gap between the word and the image—a gap the poet tries to bridge but can never close (the "Ekphrastic Hope").42 However, scholars like **Hannes Bajohr** argue that in multimodal AI, this gap has collapsed.

* **Connectionist Unity:** In a neural network (e.g., CLIP, Stable Diffusion), the text encoder and the image encoder map data to the *same* high-dimensional latent space. The vector for the word "dog" and the vector for an image of a "dog" are mathematically aligned.11  
* **Operative Nature:** The text is no longer descriptive; it is **operative**. It is the "technical substrate" that *causes* the image to exist. The text acts as code.

### **4.2 The Ontological Collapse and the "Imagetext"**

This creates a condition of **"Ontological Collapse"**.11 There is no "original" object being described. The "referent" of the prompt is the **Latent Space** itself. **W.J.T. Mitchell’s** concept of the **"Imagetext"**—a composite form where word and image are inextricable—becomes the default state of AI media.41 The prompt and the generation are two sides of the same algorithmic coin.

* **The Return of Semantics:** Historically, computing was viewed as "syntactic" (symbol manipulation without understanding). Operative Ekphrasis suggests the return of "semantics" to the digital. The *meaning* of the prompt (its semantic density) directly controls the topology of the generation.  
* **Implication for Thickness:** Because the text *is* the operative code, the "resolution" of the output is strictly limited by the "resolution" of the input. A "thin" prompt is low-resolution code, producing a low-resolution (generic) reality. A "thick" prompt is high-resolution code, triggering a high-fidelity simulation.11

### **4.3 The Intuition Pump**

Daniel Dennett defined an **"Intuition Pump"** as a thought experiment that clarifies thinking. Generative AI is the ultimate Intuition Pump for the humanities.11 By forcing us to describe the world so that a machine can simulate it, it reveals the gaps in our own understanding. If we cannot prompt "thickly" enough to generate a coherent culture, it exposes our own reliance on stereotypes and "thin" descriptions. Thick Prompting is thus a diagnostic tool for human understanding as much as a control mechanism for machines.

## **5\. Part III: The Mechanics of Thickness**

How does "thickness" exist physically within the machine? It is not just a metaphor; it is a mathematical property of the **Latent Space** and the **Transformer Architecture**.

### **5.1 The Geometry of Meaning: Latent Space as Memory Palace**

The **Latent Space** of a model is a multi-dimensional manifold (often with thousands of dimensions) where concepts are stored as vectors.

* **Peter Gärdenfors and Conceptual Spaces:** Gärdenfors argues that concepts are geometric regions in a semantic space. "Red" is a region; "Dog" is a region. These regions tend to be **convex**—meaning any point between two points in the region is also in the region.46  
* **Attractors:** In the dynamic system of the LLM, certain states act as **"Semantic Attractors"**.48 Once the generation enters the "basin of attraction" of a concept (e.g., "Shakespearean"), it tends to stay there.  
* **The Method of Loci:** The ancient mnemonic technique of the **Memory Palace** (placing ideas in spatial locations to remember them) is structurally isomorphic to how Transformers work.49 The model "places" knowledge in the vector space.  
  * **VR Evidence:** Research in Virtual Reality (VR) confirms that the "Method of Loci" works by exploiting the brain's grid cells for spatial navigation.51  
  * **The Prompt as Navigation:** A prompt is a set of coordinates. A "thin" prompt ("Dog") points to the mathematical mean (center) of the "Dog" region—resulting in a generic image. A "thick" prompt ("A stray dog with a limp waiting under a neon sign in 1980s Kowloon") provides a complex vector path that steers the model away from the mean and toward a specific, marginal, and highly detailed "room" in the Memory Palace.53

### **5.2 Positional Encoding: The Spatiality of Syntax**

The **Transformer** architecture (the "T" in GPT) processes language **spatially**, not sequentially. Because it ingests all tokens at once (parallel processing), it needs a way to understand word order.

* **Sinusoidal Encoding:** The model injects a **Positional Encoding** vector into each word embedding. These vectors are generated using **sine and cosine functions** of different frequencies:  
  ![][image1]  
  ![][image2]  
  .55  
* **Spatial Semantics:** This means the model understands grammar as a **wave-interference pattern**. The "meaning" of a sentence is a geometric interference of semantic vectors and positional waves.  
* **Thick Syntax:** "Thick Prompting" often involves complex syntax (hypotaxis, nested clauses). This creates complex positional patterns that the model’s **Attention Mechanism** can exploit to maintain long-range dependencies. A logically structured, complex sentence acts as a "stronger signal" in the spatial encoding, allowing the model to "attend" to the instructions more effectively than a list of bullet points.57

### **5.3 World Models: From Next-Token to Next-World**

**Yann LeCun** has criticized LLMs for being "Auto-Regressive" (predicting the next word) rather than having a **"World Model"** (predicting the next state of the world).12

* **JEPA Architecture:** LeCun proposes architectures like **JEPA (Joint Embedding Predictive Architecture)** that learn to predict abstract representations of the world state, ignoring irrelevant details (like pixel noise).  
* **Thick Prompting as Bridge:** Until fully autonomous World Models exist, Thick Prompting acts as a **human-in-the-loop World Model**. By explicitly describing the physics, causality, and social logic in the text prompt (the "Worldtext"), the user forces the LLM to *simulate* a World Model. We use the "latent space of language" (which LeCun admits is a powerful subspace of reality) to approximate the physics of the world.12

### **5.4 Entropy and Model Collapse**

**Model Collapse** is the tendency of generative models trained on synthetic data to lose variance and converge on the "mean" of the distribution.4 The "tails" of the distribution—the rare, the weird, the specific—are lost.

* **Entropy:** In information theory, low entropy means high predictability (cliché). High entropy means high surprise (novelty).  
* **Thick Prompting as Negative Entropy:** A thick prompt is an injection of **negative entropy** (information). By demanding specific, rare, and complex details, the user forces the model to sample from the "tails" of the distribution, preventing the collapse into the generic. It is an act of **conservation**, preserving the diversity of the latent space against the pull of the average.61

## **6\. Part IV: The "Thick Prompting" Framework**

Based on this synthesis, we can delineate a formal methodology for **Thick Prompting**. This is not merely "writing long prompts," but a structured approach to **Ethnographic** and **Semiotic** engineering.

### **6.1 Ethnographic Prompting: The Prompt Engineer as Fieldworker**

This approach re-imagines the prompt engineer as a digital ethnographer.10

* **The Field Site:** The latent space is treated as a culture to be studied.  
* **The Method:** Instead of commanding a result ("Write a poem"), the prompter describes the **cultural context** of the production.  
  * *Thin:* "Write a poem about war."  
  * *Thick:* "You are a weary soldier-poet in the trenches of the Somme, 1916\. You are writing a letter home that you know will be censored. Use the slang of the British Tommies, but encode a hidden message about the futility of the command structure using metaphors of mud and machinery."  
* **Narrative AI Ethnography (NAIE):** This method involves iterative prompting to "map" the biases and cultural boundaries of the model. By exploring how the model responds to "thick" cultural cues, the researcher can expose the "Eurocentric" or "WEIRD" (Western, Educated, Industrialized, Rich, Democratic) biases in the training data and correct for them via context injection.62

### **6.2 Semiotic Density and "Code-Switching"**

Thick Prompting utilizes **Semiotic Prompting** 64—the use of specific symbols, dialects, and jargon to signal "in-group" status to the model.

* **Shibboleths:** Using specific technical terms (e.g., "ontological collapse" instead of "reality breaking") acts as a password that unlocks a specific "expert" region of the latent space.  
* **Code-Switching:** Embedding phrases from specific languages or dialects forces the model to shift its "attention heads" to different cultural clusters, preventing the "default" American English cultural norms from dominating the output.65

### **6.3 The Logic of Layers: The "Homeric" Heuristic**

To construct a robust "Worldtext," prompts should be layered, mimicking the structure of the Shield of Achilles.21

| Layer | Function | Homeric Analog | AI Analog |
| :---- | :---- | :---- | :---- |
| **1\. Cosmology** | Defines physical laws, time, space. | The Ocean River, The Heavens | System Prompt / Physics Engine |
| **2\. Sociology** | Defines norms, roles, relations. | The City at Peace/War | Cultural Context / Ethnography |
| **3\. Economy** | Defines resources, tools, incentives. | Plowing, Harvest | Task Constraints / Mechanics |
| **4\. Narrative** | Defines specific agents and actions. | The Dance, The Debate | The User Query / Action |

**Table 1: The Homeric Layering for Thick Prompting**

By defining these layers explicitly, the prompter ensures that the "agent" (the specific output) has a coherent "world" to inhabit, reducing the likelihood of causal hallucinations (e.g., an agent using a tool that shouldn't exist in that economy).12

## **7\. Conclusion: The Poetics of the Artificial**

The shift from "Thin" to "Thick" Prompting represents the maturation of the human-AI relationship. We are moving from a phase of **discovery**—where we marveled that the machine could speak at all—to a phase of **literacy**, where we must teach it to speak with truth, depth, and cultural memory.

The prompt is the new **techné**. It is the anvil of Hephaestus, the lyre of the Bard, and the notebook of the Ethnographer. But fundamentally, it is an act of **Operative Ekphrasis**. It is the word that summons the world.

If we prompt thinly, we act as tourists in the latent space, receiving only the "kitsch" and the "souvenirs" of culture—the statistical averages that erase difference. We risk building a synthetic reality that is smooth, generic, and ultimately empty—a "Model Collapse" of the human imagination.

But if we prompt **thickly**—with the rigor of the anthropologist, the logic of the philosopher, and the vision of the poet—we can use these machines to construct "experimental models" of profound complexity. We can navigate the "Memory Palace" of the latent space to retrieve the specific, the marginal, and the beautiful. We can ensure that the "Worldtext" of the future is not a flat, gray average, but a "Shield of Achilles"—teeming with motion, conflict, life, and the "black earth" of reality, even when forged in silicon gold.

The limits of our prompts, as Wittgenstein might have said, mean the limits of our world. To build a better synthetic world, we must first learn to describe it thickly.

---

**References & Data Integration Strategy:**

* **Thick Description:** Geertz 1, Ryle.  
* **Oral Theory:** Parry/Lord 13, Varshney.16  
* **Ekphrasis/Imagetext:** Mitchell 41, Bajohr 11, Homer.21  
* **World Models/AI:** LeCun 12, Transformers 55, Latent Space.46  
* **Philosophy:** Wittgenstein 30, Sørlander 6, Austin/Searle.36  
* **Evaluation:**.3  
* **VR/Memory:**.51

#### **Works cited**

1. Using LLMs to make cultural context legible at scale \- OpenReview, accessed January 26, 2026, [https://openreview.net/pdf?id=kJfpS7lCVT](https://openreview.net/pdf?id=kJfpS7lCVT)  
2. Text as a Tool. The Effects of Using Image-Generating AI in German ..., accessed January 26, 2026, [https://digital-pedagogy.eu/text-as-a-tool-the-effects-of-using-image-generating-ai/](https://digital-pedagogy.eu/text-as-a-tool-the-effects-of-using-image-generating-ai/)  
3. A Framework for “Thick” Culture Alignment Evaluation in LLMs, accessed January 26, 2026, [https://arxiv.org/html/2511.12014v1](https://arxiv.org/html/2511.12014v1)  
4. Understanding Implosion in Text-to-Image Generative Models, accessed January 26, 2026, [https://people.cs.uchicago.edu/\~ravenben/publications/pdf/implosion-ccs24.pdf](https://people.cs.uchicago.edu/~ravenben/publications/pdf/implosion-ccs24.pdf)  
5. (DOC) Neither "Too Thick" nor "Too Thin": Decolonizing Love and ..., accessed January 26, 2026, [https://www.academia.edu/32011948/Neither\_Too\_Thick\_nor\_Too\_Thin\_Decolonizing\_Love\_and\_Language\_in\_Toni\_Morrison\_s\_Beloved](https://www.academia.edu/32011948/Neither_Too_Thick_nor_Too_Thin_Decolonizing_Love_and_Language_in_Toni_Morrison_s_Beloved)  
6. redA Philosophy Basis for Domain Science & Engineering?black, accessed January 26, 2026, [http://www.imm.dtu.dk/\~dibj/2018/Nov2018/philo.pdf](http://www.imm.dtu.dk/~dibj/2018/Nov2018/philo.pdf)  
7. Model collapse \- Wikipedia, accessed January 26, 2026, [https://en.wikipedia.org/wiki/Model\_collapse](https://en.wikipedia.org/wiki/Model_collapse)  
8. What is Thick Description in Qualitative Research? \- QDAcity, accessed January 26, 2026, [https://qdacity.com/thick-description/](https://qdacity.com/thick-description/)  
9. Clifford Geertz's "Thick Description" explained (summary), accessed January 26, 2026, [https://culturalstudiesnow.blogspot.com/2012/05/clifford-geertzs-thick-description.html](https://culturalstudiesnow.blogspot.com/2012/05/clifford-geertzs-thick-description.html)  
10. DESIGNING A FRAMEWORK FOR ETHNOGRAPHY-DRIVEN ..., accessed January 26, 2026, [https://epublications.vu.lt/object/elaba:235388584/235388584.pdf](https://epublications.vu.lt/object/elaba:235388584/235388584.pdf)  
11. Operative ekphrasis: The collapse of the text/image distinction in ..., accessed January 26, 2026, [https://www.researchgate.net/publication/372400146\_Operative\_ekphrasis\_The\_collapse\_of\_the\_textimage\_distinction\_in\_multimodal\_AI\_PLEASE\_REFER\_TO\_PUBLISHED\_VERSION](https://www.researchgate.net/publication/372400146_Operative_ekphrasis_The_collapse_of_the_textimage_distinction_in_multimodal_AI_PLEASE_REFER_TO_PUBLISHED_VERSION)  
12. Critiques of World Models \- arXiv, accessed January 26, 2026, [https://arxiv.org/pdf/2507.05169](https://arxiv.org/pdf/2507.05169)  
13. An Annotated Reading of 'The Singer of Tales' in the LLM Era \- arXiv, accessed January 26, 2026, [https://arxiv.org/html/2502.05148v1](https://arxiv.org/html/2502.05148v1)  
14. An Annotated Reading of 'The Singer of Tales' in the LLM Era, accessed January 26, 2026, [https://www.researchgate.net/publication/388848026\_An\_Annotated\_Reading\_of\_'The\_Singer\_of\_Tales'\_in\_the\_LLM\_Era](https://www.researchgate.net/publication/388848026_An_Annotated_Reading_of_'The_Singer_of_Tales'_in_the_LLM_Era)  
15. An Annotated Reading of 'The Singer of Tales' in the LLM Era \- arXiv, accessed January 26, 2026, [https://arxiv.org/pdf/2502.05148](https://arxiv.org/pdf/2502.05148)  
16. An Annotated Reading of 'The Singer of Tales' in the LLM Era \- arXiv, accessed January 26, 2026, [https://arxiv.org/abs/2502.05148](https://arxiv.org/abs/2502.05148)  
17. Why is it unlikely Homer existed? : r/classics \- Reddit, accessed January 26, 2026, [https://www.reddit.com/r/classics/comments/1ia0ld2/why\_is\_it\_unlikely\_homer\_existed/](https://www.reddit.com/r/classics/comments/1ia0ld2/why_is_it_unlikely_homer_existed/)  
18. The Anvil and the Caduceus: The Emergence of Technē's Gods, accessed January 26, 2026, [https://socialecologies.wordpress.com/2025/10/17/the-anvil-and-the-caduceus-the-emergence-of-technes-gods/](https://socialecologies.wordpress.com/2025/10/17/the-anvil-and-the-caduceus-the-emergence-of-technes-gods/)  
19. Robo-Pop: How Robots Came to Dominate Popular Culture, accessed January 26, 2026, [https://www.mouser.mx/blog/robo-pop-how-robots-came-to-dominate-popular-culture](https://www.mouser.mx/blog/robo-pop-how-robots-came-to-dominate-popular-culture)  
20. On Techne and Episteme | PDF | Plato | Aristotle \- Scribd, accessed January 26, 2026, [https://www.scribd.com/document/99709486/On-Techne-and-Episteme](https://www.scribd.com/document/99709486/On-Techne-and-Episteme)  
21. Achilles' Shield: A Symbol of Valor in Ancient Greek Warfare, accessed January 26, 2026, [https://www.greekhistoryhub.com/pages/achilles-shield-a-symbol-of-valor-in-ancient-greek-warfare-187b5525.php](https://www.greekhistoryhub.com/pages/achilles-shield-a-symbol-of-valor-in-ancient-greek-warfare-187b5525.php)  
22. The shield of Achilles in Graeco-Roman word and image, accessed January 26, 2026, [https://www.researchgate.net/publication/271821366\_Ekphrasis\_at\_the\_forge\_and\_the\_forging\_of\_ekphrasis\_The\_shield\_of\_Achilles\_in\_Graeco-Roman\_word\_and\_image](https://www.researchgate.net/publication/271821366_Ekphrasis_at_the_forge_and_the_forging_of_ekphrasis_The_shield_of_Achilles_in_Graeco-Roman_word_and_image)  
23. Is there any chance that the Shield of Achilles as described ... \- Quora, accessed January 26, 2026, [https://www.quora.com/Is-there-any-chance-that-the-Shield-of-Achilles-as-described-by-Homer-was-based-on-a-historical-artifact-of-the-Bronze-Age](https://www.quora.com/Is-there-any-chance-that-the-Shield-of-Achilles-as-described-by-Homer-was-based-on-a-historical-artifact-of-the-Bronze-Age)  
24. Word & Image: A Journal of Verbal/Visual Enquiry Ekphrasis at the ..., accessed January 26, 2026, [https://www.normalesup.org/\~jwang/files/2022/Mythe\_image/Squire\_ekphrasis.pdf](https://www.normalesup.org/~jwang/files/2022/Mythe_image/Squire_ekphrasis.pdf)  
25. VISUALIZING THE SHIELD OF ACHILLES: APPROACHING ITS ..., accessed January 26, 2026, [https://www.cambridge.org/core/journals/classical-quarterly/article/visualizing-the-shield-of-achilles-approaching-its-landscapes-via-cognitive-paths/C41663ACD10561238BF1FD6EB58E2BF1](https://www.cambridge.org/core/journals/classical-quarterly/article/visualizing-the-shield-of-achilles-approaching-its-landscapes-via-cognitive-paths/C41663ACD10561238BF1FD6EB58E2BF1)  
26. Gestural Ekphrasis: Toward a Phenomenology of the Moving Body ..., accessed January 26, 2026, [https://digitalcommons.du.edu/context/etd/article/2401/viewcontent/Benke\_denver\_0061D\_11603.pdf](https://digitalcommons.du.edu/context/etd/article/2401/viewcontent/Benke_denver_0061D_11603.pdf)  
27. Ludwig Wittgenstein: Tractatus Logico-Philosophicus, accessed January 26, 2026, [https://pressbooks.cuny.edu/philosophyashorthistory3/chapter/\_\_unknown\_\_-6/](https://pressbooks.cuny.edu/philosophyashorthistory3/chapter/__unknown__-6/)  
28. (PDF) Notion of Private Language in Wittgenstein's Tractatus Logico ..., accessed January 26, 2026, [https://www.researchgate.net/publication/348934882\_Notion\_of\_Private\_Language\_in\_Wittgenstein's\_Tractatus\_Logico-philosophcus\_and\_some\_Contemporary\_Linguistic\_Refutations](https://www.researchgate.net/publication/348934882_Notion_of_Private_Language_in_Wittgenstein's_Tractatus_Logico-philosophcus_and_some_Contemporary_Linguistic_Refutations)  
29. Do Large Language Models Defend Inferentialist Semantics ... \- arXiv, accessed January 26, 2026, [https://arxiv.org/html/2412.14501v1](https://arxiv.org/html/2412.14501v1)  
30. Physical Pictures \- CORE, accessed January 26, 2026, [https://core.ac.uk/download/pdf/295724553.pdf](https://core.ac.uk/download/pdf/295724553.pdf)  
31. Engineering Models Circa 1914 and in Wittgenstein's Tractatus, accessed January 26, 2026, [https://soar.wichita.edu/bitstreams/b5106b9e-dfe7-4a3f-9b93-2620e276c7d7/download](https://soar.wichita.edu/bitstreams/b5106b9e-dfe7-4a3f-9b93-2620e276c7d7/download)  
32. No Consciousness? No Meaning (and no AGI\!) \- Qeios, accessed January 26, 2026, [https://www.qeios.com/read/DN232Y.5](https://www.qeios.com/read/DN232Y.5)  
33. Introduction and background to symbolic logic and artificial languages, accessed January 26, 2026, [https://researchonline.rca.ac.uk/4341/1/MIT%20Press%20Language%20Games.pdf](https://researchonline.rca.ac.uk/4341/1/MIT%20Press%20Language%20Games.pdf)  
34. Wittgenstein \- Aaron's notes, accessed January 26, 2026, [https://aarnphm.xyz/thoughts/Wittgenstein](https://aarnphm.xyz/thoughts/Wittgenstein)  
35. Wittgenstein, Artificial Intelligence and the Emergence of Empathy, accessed January 26, 2026, [https://voidnetwork.gr/2020/12/28/wittgenstein-ai-and-the-emergence-of-empathy/](https://voidnetwork.gr/2020/12/28/wittgenstein-ai-and-the-emergence-of-empathy/)  
36. J L Austin and the evolution of explicit performatives | by adamhodgkin, accessed January 26, 2026, [https://adamhodgkin.medium.com/j-l-austin-and-the-evolution-of-explicit-performatives-a3babbae1411](https://adamhodgkin.medium.com/j-l-austin-and-the-evolution-of-explicit-performatives-a3babbae1411)  
37. Speech Act Theory | PDF | Semantics | Analytic Philosophy \- Scribd, accessed January 26, 2026, [https://www.scribd.com/document/515470919/Speech-Act-Theory](https://www.scribd.com/document/515470919/Speech-Act-Theory)  
38. Automating the Analysis of Speech Acts in Teams to Understand ..., accessed January 26, 2026, [https://www.researchgate.net/publication/383910156\_Automating\_the\_Analysis\_of\_Speech\_Acts\_in\_Teams\_to\_Understand\_Distributed\_Sensemaking](https://www.researchgate.net/publication/383910156_Automating_the_Analysis_of_Speech_Acts_in_Teams_to_Understand_Distributed_Sensemaking)  
39. Teaching Machines to Think: A Journey Through Prompt and ..., accessed January 26, 2026, [https://maxplanckai.medium.com/teaching-machines-to-think-a-journey-through-prompt-and-context-engineering-3d4622eb3509](https://maxplanckai.medium.com/teaching-machines-to-think-a-journey-through-prompt-and-context-engineering-3d4622eb3509)  
40. Not Minds, but Signs: Reframing LLMs through Semiotics \- arXiv, accessed January 26, 2026, [https://arxiv.org/html/2505.17080v2](https://arxiv.org/html/2505.17080v2)  
41. accessed January 26, 2026, [https://search.proquest.com/openview/62ffff89ea9ad1792e9b6de277cf3b33/1?pq-origsite=gscholar\&cbl=18750\&diss=y\#:\~:text=What%20is%20Imagetext%3F,which%20fall%20under%20this%20category.](https://search.proquest.com/openview/62ffff89ea9ad1792e9b6de277cf3b33/1?pq-origsite=gscholar&cbl=18750&diss=y#:~:text=What%20is%20Imagetext%3F,which%20fall%20under%20this%20category.)  
42. AI Generative Art as Algorithmic Remediation \- media/rep, accessed January 26, 2026, [https://mediarep.org/bitstreams/db11c2d8-a3c1-427d-911e-7bd788e5facd/download](https://mediarep.org/bitstreams/db11c2d8-a3c1-427d-911e-7bd788e5facd/download)  
43. REPRESENTATION BEYOND REPRESENTATION:, accessed January 26, 2026, [https://ufdcimages.uflib.ufl.edu/UF/E0/01/81/80/00001/UFE0018180.pdf](https://ufdcimages.uflib.ufl.edu/UF/E0/01/81/80/00001/UFE0018180.pdf)  
44. Image x Text, accessed January 26, 2026, [http://acta.bibl.u-szeged.hu/88272/1/litterae\_humaniores\_2025\_001\_015-025.pdf](http://acta.bibl.u-szeged.hu/88272/1/litterae_humaniores_2025_001_015-025.pdf)  
45. Operative Ekphrasis and the Reduction of Fashion Through ..., accessed January 26, 2026, [https://www.researchgate.net/publication/396119864\_Possible\_Fashion\_Images\_Operative\_Ekphrasis\_and\_the\_Reduction\_of\_Fashion\_Through\_Multimodal\_AI](https://www.researchgate.net/publication/396119864_Possible_Fashion_Images_Operative_Ekphrasis_and_the_Reduction_of_Fashion_Through_Multimodal_AI)  
46. The Shape of Thought: Knowledgespace \- Shep Bryan, accessed January 26, 2026, [https://www.shepbryan.com/blog/shape-of-thought-knowledgespace-intro](https://www.shepbryan.com/blog/shape-of-thought-knowledgespace-intro)  
47. Conceptual Spaces: The Geometry of Thought \- ResearchGate, accessed January 26, 2026, [https://www.researchgate.net/publication/235734026\_Conceptual\_Spaces\_The\_Geometry\_of\_Thought](https://www.researchgate.net/publication/235734026_Conceptual_Spaces_The_Geometry_of_Thought)  
48. How to Tame Your LLM: Semantic Collapse in Continuous Systems, accessed January 26, 2026, [https://arxiv.org/html/2512.05162v1](https://arxiv.org/html/2512.05162v1)  
49. Neural correlates of training-related memory improvement in ... \- PNAS, accessed January 26, 2026, [https://www.pnas.org/doi/10.1073/pnas.1735487100](https://www.pnas.org/doi/10.1073/pnas.1735487100)  
50. Explore the 'Portraits from the Memory Palace' \- A Unique Art ..., accessed January 26, 2026, [https://www.colonnacontemporary.com/memory-palace](https://www.colonnacontemporary.com/memory-palace)  
51. (PDF) The Method of Loci in Virtual Reality: Explicit Binding of ..., accessed January 26, 2026, [https://www.researchgate.net/publication/334269169\_The\_Method\_of\_Loci\_in\_Virtual\_Reality\_Explicit\_Binding\_of\_Objects\_to\_Spatial\_Contexts\_Enhances\_Subsequent\_Memory\_Recall](https://www.researchgate.net/publication/334269169_The_Method_of_Loci_in_Virtual_Reality_Explicit_Binding_of_Objects_to_Spatial_Contexts_Enhances_Subsequent_Memory_Recall)  
52. Cognitive Load-Driven VR Memory Palaces \- arXiv, accessed January 26, 2026, [https://arxiv.org/html/2506.02700v1](https://arxiv.org/html/2506.02700v1)  
53. Artificial Memory and Orienting Infinity \- Summer of Protocols, accessed January 26, 2026, [https://summerofprotocols.com/wp-content/uploads/2024/03/Artificial-Memory-and-Orienting-Infinity-Kei-Kreutler.pdf](https://summerofprotocols.com/wp-content/uploads/2024/03/Artificial-Memory-and-Orienting-Infinity-Kei-Kreutler.pdf)  
54. (PDF) Latent Structured Hopfield Network for Semantic Association ..., accessed January 26, 2026, [https://www.researchgate.net/publication/392335661\_Latent\_Structured\_Hopfield\_Network\_for\_Semantic\_Association\_and\_Retrieval](https://www.researchgate.net/publication/392335661_Latent_Structured_Hopfield_Network_for_Semantic_Association_and_Retrieval)  
55. A Gentle Introduction to Positional Encoding in Transformer Models ..., accessed January 26, 2026, [https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/](https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/)  
56. Positional Encoding in Transformers \- GeeksforGeeks, accessed January 26, 2026, [https://www.geeksforgeeks.org/nlp/positional-encoding-in-transformers/](https://www.geeksforgeeks.org/nlp/positional-encoding-in-transformers/)  
57. AGI Multimodal Cognition Blueprint Expanded \- Figshare, accessed January 26, 2026, [https://figshare.com/ndownloader/files/58943116](https://figshare.com/ndownloader/files/58943116)  
58. Neural Representations of Physics Concepts | Request PDF, accessed January 26, 2026, [https://www.researchgate.net/publication/301646733\_Neural\_Representations\_of\_Physics\_Concepts](https://www.researchgate.net/publication/301646733_Neural_Representations_of_Physics_Concepts)  
59. World Models \- junfanz1/Awesome-AI-Engineer-Review \- GitHub, accessed January 26, 2026, [https://github.com/junfanz1/AI-LLM-ML-CS-Quant-Review/blob/main/Paper%20Reading/World%20Models.md](https://github.com/junfanz1/AI-LLM-ML-CS-Quant-Review/blob/main/Paper%20Reading/World%20Models.md)  
60. Understanding World Models in AI: A Technical Guide | by Sulbha Jain, accessed January 26, 2026, [https://medium.com/@sulbha.jindal/understanding-world-models-in-ai-a-technical-guide-359bccdd174a](https://medium.com/@sulbha.jindal/understanding-world-models-in-ai-a-technical-guide-359bccdd174a)  
61. View of The gendered dress of DALL-E 2 \- MedieKultur, accessed January 26, 2026, [https://www.mediekultur.dk/article/view/143565/191756](https://www.mediekultur.dk/article/view/143565/191756)  
62. Large Language Models Struggle with Ethnographic Text Annotation, accessed January 26, 2026, [https://arxiv.org/html/2601.12099v1](https://arxiv.org/html/2601.12099v1)  
63. The Anthropology of Machines: A Digital Field Experiment (Final), accessed January 26, 2026, [https://roberteccles.com/wp-content/uploads/2025/11/The-Anthropology-of-Machines-A-Digital-Field-Experiment-Final.pdf](https://roberteccles.com/wp-content/uploads/2025/11/The-Anthropology-of-Machines-A-Digital-Field-Experiment-Final.pdf)  
64. International Conference on TESOL & Translation 2025, accessed January 26, 2026, [https://bulletin.dyu.edu.tw/file/A0208/151364.pdf](https://bulletin.dyu.edu.tw/file/A0208/151364.pdf)  
65. IU Bloomington Events for Archives, accessed January 26, 2026, [https://documentation.events.iu.edu/archives/iub.html](https://documentation.events.iu.edu/archives/iub.html)  
66. World Models Should Prioritize the Unification of Physical and ..., accessed January 26, 2026, [https://openreview.net/pdf?id=aVwhTcSMl4](https://openreview.net/pdf?id=aVwhTcSMl4)  
67. Another New Wittgenstein: The Scientific and Engineering ..., accessed January 26, 2026, [https://www.philosophie.tu-darmstadt.de/media/institut\_fuer\_philosophie/mitarbeiter\_innen/nordmann\_1/pdfs\_2/newwittgenstein.pdf](https://www.philosophie.tu-darmstadt.de/media/institut_fuer_philosophie/mitarbeiter_innen/nordmann_1/pdfs_2/newwittgenstein.pdf)

[image1]: <data:image/png;base64,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>

[image2]: <data:image/png;base64,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>